[Introduction](https://openai.com/index/techniques-for-training-large-neural-networks/)

----------

Tensor parallelism

Megatron-LM 

------

pipeline parallelism 

GPipe 

PipeDream

----

Data parallelism

[Deepspeed-zero](ZeRO: Memory Optimizations Toward Training Trillion  Parameter Model)and [ZeRO-offload](https://www.usenix.org/system/files/atc21-ren-jie.pdf)
[FSDP](https://arxiv.org/abs/2304.11277)


----
Expert parallelism (MoE)
[GShard](https://arxiv.org/abs/2006.16668)
[Switch-transformer](https://arxiv.org/abs/2101.03961)
[DeepSeek-MoE](https://arxiv.org/abs/2401.06066) 
[DeepSeek-V2 Technical Report](https://arxiv.org/abs/2405.04434), 
[Auxiliary-Loss-Free Load Balancing](https://arxiv.org/abs/2408.15664)


